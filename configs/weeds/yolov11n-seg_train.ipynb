{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Weed & Rose Detection with Ultralytics YOLO11n\n",
    "This notebook trains a YOLO11n model to detect weeds and roses from an annotated dataset of segmentation masks. It defines environment settings, configuration parameters, and the training process — with options for augmentation, reproducibility, and automatic folder management."
   ],
   "id": "cf850ae4f67baf2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Make sure the notebook kernel is the conda env: Python (yolo)\n",
    "import sys, platform, torch, os\n",
    "import albumentations as A\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"OS:\", platform.platform())\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"Albumentations version:\", A.__version__)"
   ],
   "id": "4933c4892ca46ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "PROJECT_ROOT = Path(r\"D:/Ai Systems Group\")\n",
    "DATA_YAML    = PROJECT_ROOT / \"data/weeds_yolo_mask/data.yaml\"\n",
    "OUT_DIR      = PROJECT_ROOT / \"work_dirs\"\n",
    "\n",
    "CFG = {\n",
    "    \"data\": str(DATA_YAML),\n",
    "    \"model\": \"yolo11n-seg.pt\",\n",
    "    \"epochs\": 150,\n",
    "    \"batch\": 9,\n",
    "    \"imgsz\": 1024,\n",
    "    \"workers\": 6,\n",
    "    \"device\": 0,\n",
    "    \"project\": str(OUT_DIR),\n",
    "    \"name\": \"yolov11n-seg-weeds\",\n",
    "    \"exist_ok\": False,\n",
    "    \"plots\": True,\n",
    "    \"patience\": 50,\n",
    "    \"seed\": 42,\n",
    "    # \"close_mosaic\": 10,\n",
    "    # \"cache\": \"ram\",\n",
    "}\n"
   ],
   "id": "42935f6f9d733149",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(DATA_YAML, \"r\", encoding=\"utf-8\") as f:\n",
    "    ds = yaml.safe_load(f)\n",
    "print(ds)\n"
   ],
   "id": "882897b2c109fa4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Albumentations Augmentation Pipeline\n",
    "\n",
    "This cell defines the **Albumentations-based image augmentations** that complement YOLO’s built-in transformations.\n",
    "Albumentations provides additional *pixel-level augmentations* that help the model generalize better to real-world variations in lighting, texture, and camera quality.\n",
    "\n",
    "---\n",
    "\n",
    "### What it does\n",
    "1. **Imports the Albumentations library** and defines a custom `Compose` pipeline named `albu_train`.\n",
    "2. **Applies lightweight, complementary augmentations** on top of YOLO’s geometric ones:\n",
    "   - `MotionBlur` and `MedianBlur` simulate camera shake and focus variation.\n",
    "   - `RandomBrightnessContrast` adjusts brightness and contrast to improve robustness to lighting changes.\n",
    "   - `CLAHE` (Contrast Limited Adaptive Histogram Equalization) enhances local contrast and texture details.\n",
    "   - `ToGray` occasionally converts images to grayscale so the model doesn’t over-rely on color cues.\n",
    "   - `ImageCompression` reduces quality to mimic low-end cameras or image uploads.\n",
    "   - `GaussNoise` adds small random sensor noise, improving tolerance to imperfect captures.\n",
    "3. Each transformation has a **low probability (`p`)** to prevent over-augmentation while maintaining dataset diversity.\n",
    "4. When the `albumentations` package is installed, YOLOv8 automatically detects and integrates these augmentations during training, enhancing the overall robustness of your weed-vs-rose detection model."
   ],
   "id": "3f434327b50941f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T12:05:51.210368Z",
     "start_time": "2025-11-19T12:05:51.192498Z"
    }
   },
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import albumentations as A\n",
    "import math\n",
    "\n",
    "albu_train = A.Compose([\n",
    "    A.MotionBlur(p=0.02, blur_limit=5),\n",
    "    A.MedianBlur(p=0.02, blur_limit=5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.5),\n",
    "    A.CLAHE(clip_limit=(1.0, 3.0), tile_grid_size=(8, 8), p=0.3),\n",
    "    A.ToGray(p=0.05),\n",
    "    A.ImageCompression(quality_range=(70, 100), p=0.2),\n",
    "\n",
    "    A.GaussNoise(\n",
    "        std_range=(math.sqrt(10.0) / 255.0, math.sqrt(40.0) / 255.0),\n",
    "        mean_range=(0, 0),\n",
    "        p=0.15\n",
    "    ),\n",
    "])"
   ],
   "id": "91961d30072dbaf",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Model Training\n",
    "This is the main training step of the notebook.\n",
    "It loads the YOLOv8n model, applies all the training configurations defined earlier (`CFG`), and begins the training process using the specified dataset and augmentations.\n",
    "\n",
    "---\n",
    "\n",
    "### What it does\n",
    "1. **Imports and loads the YOLO model:**\n",
    "   - `model = YOLO(CFG[\"model\"])` loads the base model architecture (`yolov8n.pt`), which is the lightweight **nano version** of YOLOv8, optimized for speed and smaller datasets.\n",
    "2. **Trains the model using** `model.train(...)` with all your specified parameters:\n",
    "   - Dataset path (`data`)\n",
    "   - Number of epochs\n",
    "   - Image size and batch size\n",
    "   - GPU device index\n",
    "   - Output directory and run name\n",
    "   - Early stopping settings (`patience`)\n",
    "   - Random seed for reproducibility\n",
    "3. **Includes multiple image augmentations** (both built-in YOLO and Albumentations-level options) to improve generalization.\n",
    "4. **Saves results automatically** in your `OUT_DIR` folder under an experiment name such as:"
   ],
   "id": "f975c12f6543ad78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(CFG[\"model\"])\n",
    "results = model.train(\n",
    "    data=CFG[\"data\"],\n",
    "    epochs=CFG[\"epochs\"],\n",
    "    imgsz=CFG[\"imgsz\"],\n",
    "    batch=CFG[\"batch\"],\n",
    "    workers=CFG[\"workers\"],\n",
    "    device=CFG[\"device\"],\n",
    "    project=CFG[\"project\"],\n",
    "    name=CFG[\"name\"],\n",
    "    exist_ok=CFG[\"exist_ok\"],\n",
    "    patience=CFG[\"patience\"],\n",
    "    seed=CFG[\"seed\"],\n",
    "    plots=True,\n",
    "    cls=1.0,\n",
    "    weight_decay = 0.001,\n",
    "    lrf = 0.0005,\n",
    "\n",
    "    # --Augmentations--\n",
    "    hsv_h=0.015, hsv_s=0.50, hsv_v=0.35,\n",
    "    degrees=15.0, translate=0.06, scale=0.20, shear=5.0,\n",
    "    perspective=0.0,\n",
    "    flipud=0.0, fliplr=0.5,\n",
    "    mosaic=0.5,\n",
    "    mixup=0.1,\n",
    "    close_mosaic=10,\n",
    "    augmentations=albu_train\n",
    ")\n",
    "results\n"
   ],
   "id": "cef0717940a4ea86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# View training curves",
   "id": "71d699cd22e1eb9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "run_dir = OUT_DIR / \"yolov11n-weeds\"\n",
    "img = run_dir / \"results.png\"\n",
    "if img.exists():\n",
    "    display(Image.open(img))\n",
    "else:\n",
    "    print(\"results.png not found yet:\", img)\n"
   ],
   "id": "b095c4b33e2d42f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Validate on val set (metrics + plots)",
   "id": "8797ed9c4d93c979"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics = model.val(\n",
    "    data=CFG[\"data\"],\n",
    "    project=str(OUT_DIR),\n",
    "    name=f\"{CFG['name']}-val\",\n",
    "    plots=True\n",
    ")\n",
    "metrics\n"
   ],
   "id": "9bafe349284da63d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Predict on some images",
   "id": "383832f9b35b600d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "VAL_IMAGES = Path(ds[\"path\"]) / ds[\"val\"]  # images/val folder\n",
    "pred = model.predict(\n",
    "    source=str(VAL_IMAGES),\n",
    "    project=str(OUT_DIR),\n",
    "    name=f\"{CFG['name']}-pred\",\n",
    "    save=True,      # save images with drawn boxes\n",
    "    save_txt=True,  # save YOLO txt predictions\n",
    "    conf=0.4       # tweak for precision/recall trade-off\n",
    ")\n",
    "pred[:2]\n"
   ],
   "id": "225d204dc45749e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Visualize predictions\n",
    "If you want to visualize the predictions that the model did, you can find them in the `work_dir` folder, in the folder that has the extension `-pred` at the end of the name."
   ],
   "id": "5e4e498242de2721"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
